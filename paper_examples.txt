Text:
October 2022
GRAPE : Knowledge Graph Enhanced Passage Reader
for Open-domain Question Answering
Abstract
A common thread of open-domain question
answering (QA) models employs a retriever-
reader pipeline that first retrieves a handful of
relevant passages from Wikipedia and then pe-
ruses the passages to produce an answer. How-
ever, even state-of-the-art readers fail to cap-
ture the complex relationships between entities
appearing in questions and retrieved passages,
leading to answers that contradict the facts.
In light of this, we propose a novel knowl-
edge Graph enhanced passage reader, namely
GRAPE , to improve the reader performance
for open-domain QA. Specifically, for each
pair of question and retrieved passage, we first
construct a localized bipartite graph, attributed
to entity embeddings extracted from the inter-
mediate layer of the reader model. Then, a
graph neural network learns relational knowl-
edge while fusing graph and contextual repre-
sentations into the hidden states of the reader
model. Experiments on three open-domain
QA benchmarks show GRAPE can improve
the state-of-the-art performance by up to 2.2
exact match score with a negligible overhead
increase, with the same retriever and retrieved
passages. Our code is publicly available at
https://github.com/jumxglhf/GRAPE.

Text:
July 2020
A Simple Framework for Contrastive Learning of Visual Representations
Abstract
This paper presents SimCLR: a simple framework
for contrastive learning of visual representations.
We simplify recently proposed contrastive self-
supervised learning algorithms without requiring
specialized architectures or a memory bank. In
order to understand what enables the contrastive
prediction tasks to learn useful representations,
we systematically study the major components of
our framework. We show that (1) composition of
data augmentations plays a critical role in defining
effective predictive tasks, (2) introducing a learn-
able nonlinear transformation between the repre-
sentation and the contrastive loss substantially im-
proves the quality of the learned representations,
and (3) contrastive learning benefits from larger
batch sizes and more training steps compared to
supervised learning. By combining these findings,
we are able to considerably outperform previous
methods for self-supervised and semi-supervised
learning on ImageNet. A linear classifier trained
on self-supervised representations learned by Sim-
CLR achieves 76.5% top-1 accuracy, which is a
7% relative improvement over previous state-of-
the-art, matching the performance of a supervised
ResNet-50. When fine-tuned on only 1% of the
labels, we achieve 85.8% top-5 accuracy, outper-
forming AlexNet with 100× fewer labels.


Text:
August 2022
Faithful Reasoning Using Large Language
Models
Although contemporary large language models (LMs) demonstrate impressive question-answering capa-
bilities, their answers are typically the product of a single call to the model. This entails an unwelcome
degree of opacity and compromises performance, especially on problems that are inherently multi-step.
To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via
a process whose causal structure mirrors the underlying logical structure of the problem. Our approach
works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs,
one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a
beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the
effectiveness of our model on multi-step logical deduction and scientific question-answering, showing
that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning
traces whose validity can be checked by the user.

Text:
March 2022
A ConvNet for the 2020s

Abstract
The “Roaring 20s” of visual recognition began with the
introduction of Vision Transformers (ViTs), which quickly
superseded ConvNets as the state-of-the-art image classifica-
tion model. A vanilla ViT, on the other hand, faces difficulties
when applied to general computer vision tasks such as object
detection and semantic segmentation. It is the hierarchical
Transformers (e.g., Swin Transformers) that reintroduced sev-
eral ConvNet priors, making Transformers practically viable
as a generic vision backbone and demonstrating remarkable
performance on a wide variety of vision tasks. However,
the effectiveness of such hybrid approaches is still largely
credited to the intrinsic superiority of Transformers, rather
than the inherent inductive biases of convolutions. In this
work, we reexamine the design spaces and test the limits of
what a pure ConvNet can achieve. We gradually “modernize”
a standard ResNet toward the design of a vision Transformer,
and discover several key components that contribute to the
performance difference along the way. The outcome of this
exploration is a family of pure ConvNet models dubbed Con-
vNeXt. Constructed entirely from standard ConvNet modules,
ConvNeXts compete favorably with Transformers in terms of
accuracy and scalability, achieving 87.8% ImageNet top-1
accuracy and outperforming Swin Transformers on COCO
detection and ADE20K segmentation, while maintaining the
simplicity and efficiency of standard ConvNets
